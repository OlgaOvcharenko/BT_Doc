% Spellchecking done with grammarly
\section{Existing Benchmarks}
% Existing benchmarks such as CP_Clean and +1 that was sent

Currently, there is no standard and generalized way of comparing data validation, error detection and repair for ML.\\
% Sorting Hat
Sorting Hat project~\cite{sortinghat} and ML Data Prep Zoo~\cite{prepzoo} aim to benchmark the automation of the data preparation in AutoML platforms: create benchmarks and labeled datasets and use ML to automate data preparation.
ML Data Prep Zoo is a public repository with labeled datasets and pre-trained ML models for data preparation tasks (e.g., schema inference, detection of anomalous categories, category deduplication, feature type inference).
The central goal is to create large labeled datasets without time-consuming manual labeling.\\
% AutoWeka and AutoSklearn
Another existing platforms such as AutoWeka~\cite{autoweka} focuses mainly on model selection, not data cleaning and preparation~\cite{prepzoo}. 
Auto-sklearn~\cite{autosklearn} also mainly concentrates on algorithm selection and hyperparameter tuning.
\\
% OpenML
OpenML~\cite{openml_suites} is a public platform for sharing datasets, algorithms, and experiments. It also contains a number of benchmark suites. The latest suite is OpenML-CC18~\cite{openml_suites} that contains frequently used OpenMl datasets from 2018.
\\
% DataCivilizer
DataCivilizer~\cite{datacivil} is an end-to-end big data management system. 
It builds a linkage graph to for the data and utilizes it to identify data relevant to user tasks. 
It integrates data cleaning process into the query processing and trades-off the query result quality with the cleaning cost. 
Automatic error detection tools are applied to detect outliers, duplicates and integrity constraints violations.
\\
% CleanML
CleanML~\cite{cleanml} is a benchmark for joint data cleaning and machine learning. This study experimented with 14 public real-world datasets without ground truth to investigate the impact of data cleaning on ML classification task. 
ML model performance is evaluated on clean/dirty data. The study shows that data cleaning is more applicable solution comparing to specific robust models, and can be valuable in both the model development and deployment steps~\cite{cleanml}.
Moreover, some errors have less impact on ML model than other (e.g, missing values imputation positively influences ML model performance, while cleaning outliers has more likely insignificant impact). 
\\
It is shown~\cite{errors, cleanml} that the percentage of errors found by current error detection techniques in real-world datasets is still well less than 100\%. 
Many errors can be spotted only by humans, but not by currently existing instruments.
Moreover, there is no universal data cleaning tool for all datasets, and thus "composite" strategy must be applied~\cite{errors}.
% Einstein
% TODO probably why WashHouse
\\WashHouse benchmark takes a real-world dataset as an input, extracts real error patterns and dependencies (e.g., functional dependencies, value ranges) from the data, scales the dataset with configurable error rates. 
Moreover, user can define own rules, constraints or even methods how to impute the error to a particular feature.
Thus, it is easier to measure the erroneousness of the scaled dataset. Moreover, WashHouse aims to do this in a distributed way.
% Input Data/ML Applications
% Structured and semi-structured data of different types
% Denormalized one or multiple tables 

% Error Classes
% Missing values (MCAR, MAR, NMAR)
% Numerical / categorical point/sequence outliers
% Duplicate data points w/ inconsistencies (tuples, multi-valued attr, attr)
% Data type, Ref integrity, FD, IND, uniqueness violations
% Data misplacements, typos (permutations, special characters, spacing)

% Metrics
% Accuracy on Test Data (pseudo-randomized, in-/out-of-domain errors) 
% Precision/Recall of corrected data errors
% Training Runtime on Train/Val Data
% Inference Runtime on Test Data


