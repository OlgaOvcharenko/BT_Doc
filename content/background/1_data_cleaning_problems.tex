% Spellchecking done with grammarly
\section{Data Cleaning Problems}
% Abstract explanation which data cleaning problems exist etc (missing values, outliers etc)
% Data validity problem. What, why is a problem
The fundamental problem of the data cleaning is data validity. Data validation refers to the process of ensuring the quality of data, its fairness and accuracy. 
Invalid, inconsistent data can bring a larger stream of issues - Garbage in , Garbage out (GIGO) concept.
Additionally, dirty data leads to wasted resources, additional cost, monetary and time losses.
% Exact problems - missing values, outliers & problem of automatization, how to prove that clean - HoloClean, when better to clean and when no
\\Data cleaning can be divided into error detection and repair of inconsistencies to improve data quality.
Errors in data appear due to:
\begin{itemize}
    \item heterogeneous data sources such as federated database systems or data warehouses (update anomalies on denormalized data, inconsistencies, multi-modal data)
    \item human errors (errors made during data collection, bias, missing or default values) 
    \item measurement or processing errors that are mostly caused by unreliable tools, hardware/software.
\end{itemize}
Common data cleaning problems are presented in Table~\ref{tab:comon_cleaning_problems}. We differentiate between error scopes: attribute/value, observation/record, feature or between multiple features. 
\\\\
A huge effort has been made in the research community to automate both error detection and especially repairing. 
It is not always can be guaranteed that the cleaning performed on the dataset is sufficient, and if there is no sufficient probabilistic evidence that the repair is correct it is better not to fix~\cite{holoclean}. 
Additionally, for most of the real-world datasets the ground truth is unknown and manual effort and domain knowledge are needed.
\\\\
Important part of the data cleaning is the principle of minimality~\cite{minimality, holoclean}. The fewer changes are introduced to the data during error repair, the less probable it is to violate integrity constraints.
\\There is a number of mechanisms / approaches to detect and fix these issues~\cite{data_cleaning_methods}. Thus, data should be analyzed, modeled, enriched, validated and debugged.

\begin{table}[!h]
\caption{\label{tab:comon_cleaning_problems}Common data cleaning problems}
\begin{tabular}{l|l}
\toprule
Problem                 & Example       \\ 
\midrule
Duplicates              & Name: \emph{Jane Smith} and \emph{Smith Jane}    \\
Uniqueness violation    & 
\begin{tabular}[c]{l}Name: \emph{Jane Smith}h, insurance: \emph{123}\\ 
                     Name: \emph{Kate James}, insurance: \emph{123}\end{tabular}  \\
\begin{tabular}[c]{l}Violation of \\ integrity constraints\end{tabular} & DoB: \emph{11-77-1975}  \\
Typos                   & City: \emph{Los Angeles}  \\
Outliers                & Values significantly different from other  \\
Missing values          & Missing cell value for the observation: \emph{NA, NaN}            \\ 
\bottomrule
\end{tabular}
\end{table}
% TODO fix table
