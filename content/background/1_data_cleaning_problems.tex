\section{Data Cleaning Problems}
\label{sec:data_cleaning_problems}

% Abstract explanation which data cleaning problems exist etc (missing values, outliers etc)
% Data validity problem. What, why is a problem
The fundamental problem of the data cleaning is data validity.
Data validation refers to the process of ensuring the quality of data, its fairness and accuracy. 
Invalid, inconsistent data can bring a larger stream of issues - for instance the Garbage in, Garbage out (GIGO) concept.
Additionally, dirty data leads to wasted resources, extra cost, and time losses.

% Exact problems - missing values, outliers & problem of automatization, how to prove that clean - HoloClean, when better to clean and when no
\textbf{Dirty data origin:} 
Data cleaning can be divided into error detection and repair of inconsistencies to improve data quality.
Errors in data appear due to:

\begin{itemize}
    \item Heterogeneous data sources such as federated database systems or
    data warehouses (update anomalies on denormalized data, inconsistencies, multi-modal data).
    \item Human errors (errors made during manual data collection and due to manually crafted data extractors, bias, missing or default values).
    \item Or measurement / processing errors that are mostly caused by unreliable tools.
\end{itemize}

\textbf{Error detection and repair problems:}
Common data cleaning problems are presented in Table~\ref{tab:comon_cleaning_problems}.
We differentiate between error scopes: attribute/value, observation/record, feature or between multiple features. 
A huge effort has been made in the research community to automate both error detection in combination with  repairing broken records. 
It can not always be guaranteed that the cleaning performed on the dataset is sufficient.
In some cases, there is no sufficient probabilistic evidence that repairing the data is better than ignoring errors for specific cells~\cite{RekatsinasCIR2017}. 
Additionally, for most of the real-world datasets, the ground truth is unknown, and manual effort and domain knowledge are needed to establish it.

\textbf{Principle of minimality:} 
An important part of data cleaning is the principle of minimality~\cite{minimality, RekatsinasCIR2017}.
If there are two consistent repairs, we aim to pick the repair with fewer value changes as it's more likely to be correct and less likely to violate integrity constraints.
There is a number of mechanisms / approaches to detect and fix these issues~\cite{data_cleaning_methods}.
Thus, data should be analyzed, modeled, enriched, validated, and debugged.

\newcolumntype{J}{p{4cm}}
\newcolumntype{F}{p{8cm}}

\begin{table}[!t]
\caption{\label{tab:comon_cleaning_problems} Common cleaning problems}
\centering
\begin{tabular}{J|F}
\toprule
\textbf{Problem}  & \textbf{Example} \\ \midrule
Duplicates & Name: \textit{Jane Smith} and \textit{Smith Jane} \\[0.2cm]

\begin{tabular}[c]{@{}l@{}}Uniqueness violation\end{tabular}                  & \begin{tabular}[c]{@{}l@{}}Name: \textit{Jane Smith}, insurance: \textit{123} \\Name: \textit{Kate James}, insurance: \textit{123}\end{tabular} \\[0.4cm]

\begin{tabular}[c]{@{}l@{}}Violation of \\ integrity constraints\end{tabular} & DoB: 11-77-1975     \\[0.6cm]

Typos & Country: \textit{Morrrocco} \\[0.2cm]

Outliers & 30 m tall person \\[0.2cm]

\begin{tabular}[c]{@{}l@{}}Missing values\end{tabular} & 
\begin{tabular}[c]{@{}l@{}}Missing cell in an observation: \\ NA, NaN, default value\end{tabular}\\
\bottomrule
\end{tabular}
\end{table}
