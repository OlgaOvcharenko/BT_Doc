\chapter{Conclusions}
% What follows
The framework proposed in this work initiates the large-scale generation of dirty data, in which original real-world observations are used to extract error patterns and create new observations.
The new data generation framework scales existing datasets while preserving error distribution and statistics.
The experiments show linear performance in distributed execution, and super linear in local execution.
The biggest data size generated is 18 GB that corresponds to a scaling factor of \numprint{65536}x.
Six different datasets with ground truth were used for experiments: five real-world and one synthetic.
Different scaling factors were tested on all datasets.
Thus, it is demonstrated that it is possible to generate large datasets while preserving statistics of a small sample of dirty data.

% Lessons Learned
% What does that mean exactly and what are the lessons learned?
The lessons learned include: Researching the context of data cleaning, studying existing state-of-the-art frameworks and algorithms to define the components of the data generator, and execution and optimization of distributed workloads.
This work lead to learning about Apache Spark intrinsic behaviour and Pandas DataFrame specialization for handling clean and  dirty data.

% Limitations and Future Work - 3 paragraphs
% This paragraph needs rework - please focus on what conclusions we can draw from the implementation and experimental results on feasibility of distributed data generation, applicability for benchmarking cleaning for ML, and remaining challenges.
It is challenging to generate datasets while maintaining statistical properties of the original dirty dataset.
From the implementation and the experimental results we can draw a conclusion that it is feasible to generate large-scale datasets with the distributed data generator, but it can be time-consuming because of skewed runtime of tasks in central Spark jobs.
Our data generator can be applied for benchmarking cleaning for ML tools.
Interesting directions for future work include 
(1) understanding of existing skew of Spark tasks and optimization of the distributed execution,
(2) optimization of the local execution, 
(3) more advanced and fine-grained error detection and classification algorithms,
(4) support of more error types, 
(5) integration into data cleaning benchmarks, and
(6) different techniques for scaling up such as random choice of tuples from the clean dataset.
 

% % Say what your solution achieves (200 words)
% Hence, this work focuses on data generation, observations of the original clean and dirty data are used to scale up to arbitrary data sizes.
% The biggest scaling factor achieved is 65536x, and the largest data size achieved was 18 GB.
% These results were gained under the constraints of preserving data statistics and error distribution, and, therefore, new algorithms were applied.
% The generator can run either locally, or distributed via Apache Spark.
% As expected, the local execution is faster for small scaling factors up to 42x, 
% while, on the other hand, when scaling to larger sizes the distributed execution performs better.
% % Say what follows from your solution (25 words)
% Starting out, it was challenging to introduce errors preserving characteristics of the original dataset. 
% % since such errors as e.g. outliers change the mean drastically.
% The proposed framework solves the large-scale data generation problem, and provides a solution for cleaning for ML benchmarks by scaling data and its distribution.
