% Checked with grammarly
\section{Types of Errors}
\label{sec:error_types}

Data errors are values that differ from the ground truth.
In this context, we differentiate between data with and without errors by calling it clean or dirty. 
Completely clean data is ground truth (GT).  


In the context of this benchmark, we have a clean ground-truth dataset and a dirty version with introduced errors.  
Leveraging this, faulty values can be found by locating the cells that differ between dirty and clean. 
This information is helpful to detect errors and their distribution, and frequencies of specific values being error prone.  
Moreover, it is useful for classification of error types.


\textbf{Five error types}, that are common in the real-world datasets, are considered in this benchmark: 
Missing values, typos, outliers, replacements and swaps. 
The error types and descriptions are shown in Table~\ref{tab:generator_errors}.


\textbf{Missing value} is the simplest type of error that is frequently seen in our experimentation. 
Generally, there are three sub categories of missing values: 
Missing Completely at Random (MCAR), Missing at Random (MAR), Missing Not at Random (MNAR). 
MCAR is a value that is missing completely independently from other values. 
Missing values have no dependencies or ties to any other values in the dataset. 
MAR occurs when the missing value is random, but is related to the part of the observed data, an instance of this is where in datasets with personal weight, females are less inclined to include their weight. 
MNAR means that the missing value of an observation depends on its values, for instance, using the weight example again, weight is not reported for obese individuals.
MNAR analysis is problematic because the distribution of the missing observations depends on both observed and unobserved values.
Missing data introduce various problems. 
First, the absence of data reduces statistical power of the dirty dataset, moving it's statistical properties further from ground truth.
Second, the lost data can cause bias in the estimation of parameters. The previous example of people not reporting if they are obese could be a cause of this. 
Third, the representativeness of the samples can influence the set of distinct values, frequencies, and ratios between them.


The benchmark supports MCAR and MAR. 
% TODO how we maintain these properties


\textbf{Typo} is a typographical error that typically is introduced by misspelling. 
Common instances of typos are \textsc{Morrocco} and \textsc{Lost Angeles}. 
Introducing typos can violate the set of distinct values, since new distinct values can be inserted.
This can be detrimental for techniques such as one-hot-encoding that is highly sensitive to the number of distinct values.
In the benchmark, the typo distribution and the distinct value set are used to estimate the number of new unique typos to add to the generated output. 
The new distinct typos are generated by modifying existing values. Modifications are controlled by measuring Levenshtein distance between the clean and modified values. 


\textbf{Outlier} is a data point that significantly differs from a data distribution.
An example of this could be human adult height equal to 3m, 20cm, or a negative value such as -1.5m. 
Since outliers increase variability in data, they decrease statistical power.
In our benchmark we are introducing outliers while preserving dirty data statistics. 
New outlier values are created using correction, in Equation~\ref{eq:outlier_correction}. 
Correction is used to shift actual mean of the scaled data to the desired mean of the dirty data. 
To introduce outliers preserving the statistics, every new outlier should be balanced by another outlier. 
We generate outlier value by either interquartile range, or shifting delimiter, or distribution, or minimum and maximum. Then the value to balance it new value is mirrored relative to 

and mirror it  that
we need to set up maximal distance to we reflect upper and lower limits that define outliers  
\begin{equation}
\label{eq:outlier_correction}
\textsc{correction} = \frac{(\textsc{mean\_dirty} - \textsc{mean\_generated}) \cdot \textsc{nrow\_generated})}{\textsc{num\_outliers}}
\end{equation}
% TODO other explanation how we limit outlier with correction

% # Recompute limits for later balancing: max distance to mean, holds for both sides
%         dist_from_mean_to_outliers = max(abs(upper_limit - err_dist.dirty_mean[col_name]), abs(err_dist.dirty_mean[col_name] - lower_limit))
%         lower_limit, upper_limit = err_dist.clean_mean[col_name] - dist_from_mean_to_outliers, err_dist.clean_mean[col_name] + dist_from_mean_to_outliers

%         # Reflect around zero and subtract correction
%         distance_to_zero = max(abs(lower_limit), abs(upper_limit))
%         lower_limit, upper_limit = (-1 * distance_to_zero) - correction, distance_to_zero - correction

\textbf{Replacement} is a flipped value that was chosen from the existing set of valid values. 
For example, in feature with distinct set of values \textsc{\{A, B, C\}}, \textsc{A} can be replaced by \textsc{D}. 
Replacement does not introduce new distinct values, but change the frequencies of the different distinct values. 
It reduces statistical power of the dirty dataset and moves it's statistical properties further from ground truth.
In the benchmark, to preserve statistical properties, correspondence pairs of replacements and their frequencies are detected, and scaled randomly. 


\textbf{Swap} is an exchange of pair of values within a tuple. 
Mostly, they occur when data is misplaced while entering.
For instance, a \textsc{DoB} value \textsc{01.01.2001} is mistakenly filled into \textsc{Surname}, and \textsc{Surname} value \textsc{Smith} - into \textsc{DoB}.
Swaps introduce new irrelevant distinct items or outliers if two numerical values are swapped.
In the benchmark, swaps are generated based on their distribution in the dirty dataset. Swaps are detected and counted, then the number of swaps is scaled. This approach allows to maintain data distribution while generating new dataset, without introducing violating distinct value set of the dirty data.

\textbf{Single source dataset} is used to introduce errors. 
Thus multiple data source datasets and errors that occur during schema integration are not considered in the benchmark.
The error generation includes properties such as:
\begin{itemize}
    \item Error type: A variety of different error types can be introduced to the data.
    \item Error distribution: Respective error is reproduced in the scaled dataset according to the dirty data error distribution.
    \item Reproducibility: Each and every introduced error can be reproduced and tracked.
\end{itemize}

% TODO bias
\textbf{Bias}
It is important to notice that there is a potential bias in the error introduction since

% TODO in-estimation
\textbf{In-estimation} 
How we estimate what to introduce.
