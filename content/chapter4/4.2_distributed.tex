\subsection{Distributed Experiments}
\label{sec:runtime_distributed}

Table~\ref{tab:distributed_runtimes} shows the runtimes of different scaling factors in the distributed version of the data generator.
The scaling factors were increased until the execution time exceeded 12 hours per dataset.
It can be observed that some datasets scale better than others, but that is mainly because sizes are not as big as \textit{tax} yet. 
The complexity of Spark job is dominated by joins and repartitioning.
The number of joins is based on error types introduced and amount of faulty columns in total.
The joins are performed on sequential unique indices.
The \textit{tax} contains all 5 types of errors what leads to more complicated Spark jobs and more joins.
Additionally, the runtime of tasks in central Spark jobs is highly skewed (seconds vs hours) leading to poor utilization of parallel resources.
This leads to the performance that is worse by many orders of magnitude than expected.
Also, the Spark Catalyst Optimizer tries to optimize the large sequences of instructions, this leads to long startup, compile and optimization times. 
For instance, in scaling \textit{beers} to 2x the Spark context start up takes 25 seconds, data is scaled up (lazy evaluation) in 11 seconds, errors are saved to HDFS in 5 seconds, and new error distribution/statistics is computed in 11 seconds, and the result dataset is saved to HDFS in 25 seconds, plus various overheads of 7 seconds.
According to the results, the proposed distributed data generator scales linearly.

\textbf{Random error generation:} 
A faster option would be to randomly generate errors based on crafted probabilities (according to the error distribution) while replicating the individual tuples, avoiding the need to large joins altogether.
% randomly generate errors and introduce them by udfs with random behaviour seeded on index.
Although, in this case, choice of error type and preserving the error distribution is not guaranteed as in the proposed solutions.

\textbf{Generation at large scales:} 
The largest scaling factor run is \numprint{65536}x on \textit{rayyan} dataset. 
The resulting datasize is 17.9 GB, the runtime is \numprint{17711} seconds (equivalent to approximately 5 hours).

\begin{table}[!t]
\caption{\label{tab:distributed_runtimes}Distributed runtimes [s] with different scales}
\centering
\begin{tabular}{r|K|K|K|K|K|K}
\toprule
\textbf{Scale} & \colCenter{beers} & \colCenter{flights} & \colCenter{hospital} & \colCenter{movies} & \colCenter{rayyan} & \colCenterNoRight{tax} \\ \midrule
   \numprint{8} &   \numprint{85.5} &  \numprint{96.8} &   \numprint{86.4} &  \numprint{118.3}     &  \numprint{84.7}     &   \numprint{582.5} \\
  \numprint{64} &   \numprint{92.7} & \numprint{102.9} &   \numprint{93.5} &  \numprint{262.4}     &  \numprint{89.9}     &  \numprint{2978.0} \\
 \numprint{256} &  \numprint{177.7} & \numprint{164.0} &  \numprint{147.0} &  \numprint{767.0}     & \numprint{116.6}     & \numprint{11369.3} \\
\numprint{1024} &  \numprint{536.4} & \numprint{364.2} &  \numprint{378.7} & \numprint{1799.1}     & \numprint{276.4}     & \numprint{48499.5} \\
\numprint{2048} &  \numprint{923.7} &   ---            &  \numprint{661.3} & \numprint{3785.6}     & \numprint{454.3}     &  --- \\
\numprint{8192} & \numprint{2769.4} &   ---            & \numprint{2061.6} & \numprint{14512.8}    & \numprint{1591.7}   & --- \\
\bottomrule
\end{tabular}
\end{table}
